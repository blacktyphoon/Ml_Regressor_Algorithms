# -*- coding: utf-8 -*-
"""diamondPricePredictWithPipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12LP1iDCD3BkSB7PCIADAY6cg01neFI0a
"""

#Same procedure but using pipeline

"""**Define objective** : Predict diamond pricing

**What solutions do you have in place** :Mannual price calculation

**How will you measure performance :** Using RMSE

**Minimum performance required** : R2 score -> 0.95

**Machine Learning Approch** : Supervised -Regression


"""

import pandas as pd
import numpy as np

#import datasets
df = pd.read_csv("/content/diamonds.csv")

"""###Data Exploratory analysis"""

#size and variables
df.info(memory_usage='deep')

"""Based on .info() there are 11 variable, 3 categorigal and rest are numerical values.Out of 11 variables 10 are independent and 1 is dependent variable (Price)"""

df.shape

df.head()

df.columns

#Missing values check

df.isnull().sum(axis=0).sort_values(ascending=False)

"""As for null we have four numerical variable has missing values where the percent is less then 0.5%"""

#by doing mean it will give you percentage
df.isnull().mean().sort_values(ascending=False)

#Statistical description
df.describe()

"""Based on the table above ,we notice that  scaling may be required because min value  ranges from 1-326, if doing dimenson reducution

"""

#Checing for outliers
df.skew().sort_values(ascending=False)

"""We have some outliers to deal with in y,z,carat and table"""

df.kurt().sort_values(ascending=False)

"""**Fact** : when we have skew and kurtosis is 0 then the disribution is normal"""

#Unnamed: 0 is suspecious
df.nunique()

#Stasitical description of categorical values
df.describe(include='O')

#Find Correlation with df.corr()

df.corr()

import seaborn as sns

sns.heatmap(df.corr(),annot=True)

"""When correlation is soo small we can discard the variables.

We can see carret is highly corelated with y,x,z (x,y,z dimenssion of diamond)

###Data Preperation

1. Drop unnecessary columns
2. Deal with missing values
3. Categorical (rare and transform to numerical)
4. outliers
5. Scaling and Normailzation

Order of Trasnformation
1. Imputer :Remove mising values from numerical and ategorical variable

2. Apply rare level encoder , risk that some cat val show on test and not on train set

3. categorical encoding ex : one hot,ordinal

4. handling outlier data using Winsoser

5. Dimenssional Reduction
"""

#Drop non-useful columns

df.drop(columns="Unnamed: 0",inplace=True)

#Before starting any feature engineering always Split data in train and test

X = df.drop(columns="price")
y = df.price

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)

"""

1. missing values treatmemt
2. we can replace by mean,mode,median
3. if skewness is inbetween -0.5 to 0.5 we can use mean to replace
4. otherwise we use median to replace missing values
5. Because Mean is effected by skew but median not

"""

#install required library
!pip install feature-engine

#Replace null values with median
#By Default MeanMedianImputer search for all variable containing missing value and replace by the median
from feature_engine.imputation import MeanMedianImputer
imputer = MeanMedianImputer()

#Replace categorical missing values with 'missing' or frequent
from feature_engine.imputation import CategoricalImputer
cat_imp = CategoricalImputer()


#Rare Encoding is used when you have high cardinality i.e high unique values
from feature_engine.encoding import RareLabelEncoder
rare_enc = RareLabelEncoder()


#Categorical Data Encoding

from feature_engine.encoding import OrdinalEncoder
cat_enc = OrdinalEncoder()

#Transforming outliers

from feature_engine.outliers import Winsorizer
capper = Winsorizer(capping_method='iqr',tail='both')

#Data Scaling 
#Robust scaler add math trick to be little sensitive with outliers

from sklearn.preprocessing import StandardScaler, RobustScaler
scaler = StandardScaler()

"""#Never Fit on test data , transform on train and test

Cadinality : in column cut

High cardinality you have data on test but not in train i.e 4 value in train but 5 value in test means there is one variable we did not learn for.

With rare encoding will do is that for all those cat where freq is low freq

If skewness is +ve then use winsoriser with right quentile

Skewness reduced
"""

#Transformer Pipeline

#Pipeline and make_pipeline are same but Pipeleine can do more things
#In pipeline is a list of map in which map contains (transformer_name(User defined),transformer_object)

"""##Pipeline:

1. Creating pipeline
2. We should have atlest 4-5 trasformer i.e Missing Values,
3. Scaling Cat_encoder,normalization,Dim reduction
4. pipe = make_pipeline(tr1,tr2,...,trn,algo )
5. When we have new data we will do pipe.transform(X_test)
6. pipe.predict(newDate)
7. pipe.fit(x_train,y_train)
"""

from sklearn.pipeline import Pipeline,make_pipeline

transformer = make_pipeline (imputer,cat_imp,rare_enc,cat_enc,capper,scaler)

#learn from train data
transformer.fit(X_train,y_train)

#Apply Transformer to train and test
X_train = transformer.transform(X_train)
X_test = transformer.transform(X_test)

import pickle
with open('diamond_transformer.pkl','wb') as f:
  pickle.dump(transformer,f)

"""##Algorithms To Try
1. Linear (ElasticNet,Linear Model)

2. Support vector (svm,Linear SVC, NuSvc)
        They will draw a line that will seperate your   different data 

3. Tree Based (DT,Random Forest,XGBoost,ADABoost,CatBoost)

4. Distance Based Algo(Nearest Neighbour,K-Nearest Neighbour)

5. Bayesian Algo (Gaussian,Multinomial,Bernoulli)

6. Neural Network (Multilayer)

"""

!pip install catboost

#Importing Algorithm

from sklearn.linear_model import ElasticNet
from sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor,GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import LinearSVR,SVR,NuSVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from xgboost import XGBRFRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

#Create a table that has different algo and score 

algorithms = []
algorithms.append(('ElasticNet',ElasticNet()))
algorithms.append(('AdaBoost',AdaBoostRegressor()))
algorithms.append(('Random_Forest',RandomForestRegressor()))
algorithms.append(('Gradient_Boost',GradientBoostingRegressor()))
algorithms.append(('KNN_Reg',KNeighborsRegressor()))
algorithms.append(('Linear_SVR',LinearSVR()))
algorithms.append(('SVR',SVR()))
algorithms.append(('NuSVR',NuSVR()))
algorithms.append(('Decision_Tree',DecisionTreeRegressor()))
algorithms.append(('Neural_Network',MLPRegressor()))
algorithms.append(('XG_Boost',XGBRFRegressor()))
algorithms.append(('LG_Boost',LGBMRegressor()))
algorithms.append(('Cat_Boost',CatBoostRegressor()))


names = []
train_rmse = []
test_rmse = []
train_r2 = []
test_r2 = []

for name,reg in algorithms:
  reg.fit(X_train,y_train)
  train_rmse.append(sqrt(mean_squared_error(y_train,reg.predict(X_train))))
  test_rmse.append(sqrt(mean_squared_error(y_test,reg.predict(X_test))))
  train_r2.append(r2_score(y_train,reg.predict(X_train)))
  test_r2.append(r2_score(y_test,reg.predict(X_test)))
  names.append(name)


reg_comp = pd.DataFrame({'Algo':names,
                         'Train RMSE':train_rmse,
                         'Test RMSE':test_rmse,
                         'Train_R2':train_r2,
                         'Test_R2':test_r2
                         })


reg_comp.sort_values(by=['Test RMSE'])

"""Select The two best one and start hyper parameter tunning"""